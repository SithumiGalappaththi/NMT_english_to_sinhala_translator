{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2025-06-14T08:06:48.299341Z",
          "iopub.status.busy": "2025-06-14T08:06:48.298757Z",
          "iopub.status.idle": "2025-06-14T09:10:23.128625Z",
          "shell.execute_reply": "2025-06-14T09:10:23.127804Z",
          "shell.execute_reply.started": "2025-06-14T08:06:48.299317Z"
        },
        "id": "83KwQb-BwZRl",
        "outputId": "63d0d378-f015-4d73-8d7d-b1cd9ad2d0e5",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset...\n",
            "Cleaning data...\n",
            "Building model...\n",
            "Training model...\n",
            "Epoch 1/30\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - loss: 4.6171 - singlish_output_accuracy: 0.7948 - singlish_output_loss: 2.2069 - sinhala_output_accuracy: 0.7879 - sinhala_output_loss: 2.4102\n",
            "Epoch 1: val_loss improved from inf to 3.12991, saving model to multitask_nmt_best.h5\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 470ms/step - loss: 4.6148 - singlish_output_accuracy: 0.7948 - singlish_output_loss: 2.2058 - sinhala_output_accuracy: 0.7879 - sinhala_output_loss: 2.4090 - val_loss: 3.1299 - val_singlish_output_accuracy: 0.8061 - val_singlish_output_loss: 1.4996 - val_sinhala_output_accuracy: 0.7993 - val_sinhala_output_loss: 1.6294 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - loss: 3.0730 - singlish_output_accuracy: 0.8071 - singlish_output_loss: 1.4768 - sinhala_output_accuracy: 0.8001 - sinhala_output_loss: 1.5962\n",
            "Epoch 2: val_loss improved from 3.12991 to 3.07534, saving model to multitask_nmt_best.h5\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 459ms/step - loss: 3.0730 - singlish_output_accuracy: 0.8071 - singlish_output_loss: 1.4768 - sinhala_output_accuracy: 0.8001 - sinhala_output_loss: 1.5962 - val_loss: 3.0753 - val_singlish_output_accuracy: 0.8075 - val_singlish_output_loss: 1.4706 - val_sinhala_output_accuracy: 0.8001 - val_sinhala_output_loss: 1.6038 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 435ms/step - loss: 2.9979 - singlish_output_accuracy: 0.8070 - singlish_output_loss: 1.4406 - sinhala_output_accuracy: 0.7998 - sinhala_output_loss: 1.5573\n",
            "Epoch 3: val_loss improved from 3.07534 to 3.03242, saving model to multitask_nmt_best.h5\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 459ms/step - loss: 2.9979 - singlish_output_accuracy: 0.8070 - singlish_output_loss: 1.4406 - sinhala_output_accuracy: 0.7998 - sinhala_output_loss: 1.5573 - val_loss: 3.0324 - val_singlish_output_accuracy: 0.8077 - val_singlish_output_loss: 1.4510 - val_sinhala_output_accuracy: 0.8008 - val_sinhala_output_loss: 1.5805 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - loss: 2.9053 - singlish_output_accuracy: 0.8091 - singlish_output_loss: 1.3971 - sinhala_output_accuracy: 0.8018 - sinhala_output_loss: 1.5082\n",
            "Epoch 4: val_loss improved from 3.03242 to 2.97031, saving model to multitask_nmt_best.h5\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 459ms/step - loss: 2.9053 - singlish_output_accuracy: 0.8091 - singlish_output_loss: 1.3971 - sinhala_output_accuracy: 0.8018 - sinhala_output_loss: 1.5082 - val_loss: 2.9703 - val_singlish_output_accuracy: 0.8110 - val_singlish_output_loss: 1.4209 - val_sinhala_output_accuracy: 0.8040 - val_sinhala_output_loss: 1.5485 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - loss: 2.7818 - singlish_output_accuracy: 0.8139 - singlish_output_loss: 1.3384 - sinhala_output_accuracy: 0.8067 - sinhala_output_loss: 1.4434\n",
            "Epoch 5: val_loss improved from 2.97031 to 2.84496, saving model to multitask_nmt_best.h5\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 459ms/step - loss: 2.7818 - singlish_output_accuracy: 0.8139 - singlish_output_loss: 1.3384 - sinhala_output_accuracy: 0.8067 - sinhala_output_loss: 1.4434 - val_loss: 2.8450 - val_singlish_output_accuracy: 0.8165 - val_singlish_output_loss: 1.3601 - val_sinhala_output_accuracy: 0.8094 - val_sinhala_output_loss: 1.4840 - learning_rate: 0.0010\n",
            "Epoch 6/30\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - loss: 2.6059 - singlish_output_accuracy: 0.8195 - singlish_output_loss: 1.2563 - sinhala_output_accuracy: 0.8125 - sinhala_output_loss: 1.3496\n",
            "Epoch 6: val_loss improved from 2.84496 to 2.76400, saving model to multitask_nmt_best.h5\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 459ms/step - loss: 2.6059 - singlish_output_accuracy: 0.8195 - singlish_output_loss: 1.2563 - sinhala_output_accuracy: 0.8125 - sinhala_output_loss: 1.3496 - val_loss: 2.7640 - val_singlish_output_accuracy: 0.8188 - val_singlish_output_loss: 1.3221 - val_sinhala_output_accuracy: 0.8124 - val_sinhala_output_loss: 1.4409 - learning_rate: 0.0010\n",
            "Epoch 7/30\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - loss: 2.4290 - singlish_output_accuracy: 0.8239 - singlish_output_loss: 1.1727 - sinhala_output_accuracy: 0.8167 - sinhala_output_loss: 1.2563\n",
            "Epoch 7: val_loss improved from 2.76400 to 2.68099, saving model to multitask_nmt_best.h5\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 459ms/step - loss: 2.4290 - singlish_output_accuracy: 0.8239 - singlish_output_loss: 1.1727 - sinhala_output_accuracy: 0.8167 - sinhala_output_loss: 1.2563 - val_loss: 2.6810 - val_singlish_output_accuracy: 0.8212 - val_singlish_output_loss: 1.2812 - val_sinhala_output_accuracy: 0.8151 - val_sinhala_output_loss: 1.3987 - learning_rate: 0.0010\n",
            "Epoch 8/30\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - loss: 2.2258 - singlish_output_accuracy: 0.8286 - singlish_output_loss: 1.0778 - sinhala_output_accuracy: 0.8217 - sinhala_output_loss: 1.1480\n",
            "Epoch 8: val_loss improved from 2.68099 to 2.59549, saving model to multitask_nmt_best.h5\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 459ms/step - loss: 2.2259 - singlish_output_accuracy: 0.8286 - singlish_output_loss: 1.0778 - sinhala_output_accuracy: 0.8217 - sinhala_output_loss: 1.1480 - val_loss: 2.5955 - val_singlish_output_accuracy: 0.8217 - val_singlish_output_loss: 1.2381 - val_sinhala_output_accuracy: 0.8143 - val_sinhala_output_loss: 1.3563 - learning_rate: 0.0010\n",
            "Epoch 9/30\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - loss: 2.0360 - singlish_output_accuracy: 0.8317 - singlish_output_loss: 0.9893 - sinhala_output_accuracy: 0.8252 - sinhala_output_loss: 1.0467\n",
            "Epoch 9: val_loss improved from 2.59549 to 2.53828, saving model to multitask_nmt_best.h5\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 459ms/step - loss: 2.0360 - singlish_output_accuracy: 0.8317 - singlish_output_loss: 0.9893 - sinhala_output_accuracy: 0.8252 - sinhala_output_loss: 1.0467 - val_loss: 2.5383 - val_singlish_output_accuracy: 0.8236 - val_singlish_output_loss: 1.2127 - val_sinhala_output_accuracy: 0.8174 - val_sinhala_output_loss: 1.3245 - learning_rate: 0.0010\n",
            "Epoch 10/30\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - loss: 1.8540 - singlish_output_accuracy: 0.8362 - singlish_output_loss: 0.9057 - sinhala_output_accuracy: 0.8307 - sinhala_output_loss: 0.9484\n",
            "Epoch 10: val_loss improved from 2.53828 to 2.51807, saving model to multitask_nmt_best.h5\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 459ms/step - loss: 1.8541 - singlish_output_accuracy: 0.8362 - singlish_output_loss: 0.9057 - sinhala_output_accuracy: 0.8307 - sinhala_output_loss: 0.9484 - val_loss: 2.5181 - val_singlish_output_accuracy: 0.8240 - val_singlish_output_loss: 1.2054 - val_sinhala_output_accuracy: 0.8174 - val_sinhala_output_loss: 1.3114 - learning_rate: 0.0010\n",
            "Epoch 11/30\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - loss: 1.6744 - singlish_output_accuracy: 0.8427 - singlish_output_loss: 0.8212 - sinhala_output_accuracy: 0.8383 - sinhala_output_loss: 0.8532\n",
            "Epoch 11: val_loss improved from 2.51807 to 2.47969, saving model to multitask_nmt_best.h5\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 459ms/step - loss: 1.6745 - singlish_output_accuracy: 0.8427 - singlish_output_loss: 0.8213 - sinhala_output_accuracy: 0.8383 - sinhala_output_loss: 0.8532 - val_loss: 2.4797 - val_singlish_output_accuracy: 0.8228 - val_singlish_output_loss: 1.1854 - val_sinhala_output_accuracy: 0.8167 - val_sinhala_output_loss: 1.2930 - learning_rate: 0.0010\n",
            "Epoch 12/30\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - loss: 1.5534 - singlish_output_accuracy: 0.8470 - singlish_output_loss: 0.7635 - sinhala_output_accuracy: 0.8432 - sinhala_output_loss: 0.7898\n",
            "Epoch 12: val_loss improved from 2.47969 to 2.47868, saving model to multitask_nmt_best.h5\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 459ms/step - loss: 1.5534 - singlish_output_accuracy: 0.8470 - singlish_output_loss: 0.7636 - sinhala_output_accuracy: 0.8432 - sinhala_output_loss: 0.7898 - val_loss: 2.4787 - val_singlish_output_accuracy: 0.8248 - val_singlish_output_loss: 1.1843 - val_sinhala_output_accuracy: 0.8191 - val_sinhala_output_loss: 1.2932 - learning_rate: 0.0010\n",
            "Epoch 13/30\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - loss: 1.4553 - singlish_output_accuracy: 0.8510 - singlish_output_loss: 0.7156 - sinhala_output_accuracy: 0.8473 - sinhala_output_loss: 0.7397\n",
            "Epoch 13: val_loss improved from 2.47868 to 2.46872, saving model to multitask_nmt_best.h5\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 459ms/step - loss: 1.4553 - singlish_output_accuracy: 0.8510 - singlish_output_loss: 0.7156 - sinhala_output_accuracy: 0.8473 - sinhala_output_loss: 0.7397 - val_loss: 2.4687 - val_singlish_output_accuracy: 0.8223 - val_singlish_output_loss: 1.1814 - val_sinhala_output_accuracy: 0.8166 - val_sinhala_output_loss: 1.2861 - learning_rate: 0.0010\n",
            "Epoch 14/30\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - loss: 1.3632 - singlish_output_accuracy: 0.8552 - singlish_output_loss: 0.6706 - sinhala_output_accuracy: 0.8518 - sinhala_output_loss: 0.6926\n",
            "Epoch 14: val_loss did not improve from 2.46872\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 457ms/step - loss: 1.3633 - singlish_output_accuracy: 0.8552 - singlish_output_loss: 0.6706 - sinhala_output_accuracy: 0.8518 - sinhala_output_loss: 0.6927 - val_loss: 2.4841 - val_singlish_output_accuracy: 0.8232 - val_singlish_output_loss: 1.1887 - val_sinhala_output_accuracy: 0.8171 - val_sinhala_output_loss: 1.2942 - learning_rate: 0.0010\n",
            "Epoch 15/30\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - loss: 1.2884 - singlish_output_accuracy: 0.8590 - singlish_output_loss: 0.6338 - sinhala_output_accuracy: 0.8557 - sinhala_output_loss: 0.6546\n",
            "Epoch 15: val_loss did not improve from 2.46872\n",
            "\n",
            "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 457ms/step - loss: 1.2884 - singlish_output_accuracy: 0.8590 - singlish_output_loss: 0.6338 - sinhala_output_accuracy: 0.8557 - sinhala_output_loss: 0.6546 - val_loss: 2.5046 - val_singlish_output_accuracy: 0.8257 - val_singlish_output_loss: 1.1992 - val_sinhala_output_accuracy: 0.8191 - val_sinhala_output_loss: 1.3040 - learning_rate: 0.0010\n",
            "Epoch 16/30\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 437ms/step - loss: 1.1752 - singlish_output_accuracy: 0.8661 - singlish_output_loss: 0.5786 - sinhala_output_accuracy: 0.8630 - sinhala_output_loss: 0.5966\n",
            "Epoch 16: val_loss did not improve from 2.46872\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 457ms/step - loss: 1.1752 - singlish_output_accuracy: 0.8661 - singlish_output_loss: 0.5786 - sinhala_output_accuracy: 0.8630 - sinhala_output_loss: 0.5966 - val_loss: 2.5252 - val_singlish_output_accuracy: 0.8233 - val_singlish_output_loss: 1.2107 - val_sinhala_output_accuracy: 0.8173 - val_sinhala_output_loss: 1.3132 - learning_rate: 5.0000e-04\n",
            "Epoch 17/30\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 437ms/step - loss: 1.0727 - singlish_output_accuracy: 0.8734 - singlish_output_loss: 0.5308 - sinhala_output_accuracy: 0.8715 - sinhala_output_loss: 0.5418\n",
            "Epoch 17: val_loss did not improve from 2.46872\n",
            "\n",
            "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 457ms/step - loss: 1.0727 - singlish_output_accuracy: 0.8734 - singlish_output_loss: 0.5309 - sinhala_output_accuracy: 0.8715 - sinhala_output_loss: 0.5418 - val_loss: 2.5803 - val_singlish_output_accuracy: 0.8229 - val_singlish_output_loss: 1.2372 - val_sinhala_output_accuracy: 0.8166 - val_sinhala_output_loss: 1.3417 - learning_rate: 5.0000e-04\n",
            "Epoch 17: early stopping\n",
            "Restoring model weights from the end of the best epoch: 13.\n",
            "Saving model and tokenizers...\n",
            "English: how are you\n",
            "Sinhala Prediction: ඔයා කොහොමද\n",
            "Sinhala Confidence: 0.9845814\n",
            "Singlish Prediction: oya kohomada\n",
            "Singlish Confidence: 0.98286253\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, RepeatVector, TimeDistributed, Dense, Concatenate, AdditiveAttention\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from pickle import dump\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "\n",
        "# 1. Data Cleaning\n",
        "def clean_text(text, is_sinhala=False):\n",
        "    if not is_sinhala:\n",
        "        text = text.lower()\n",
        "        text = re.sub(r\"[%s]\" % re.escape(string.punctuation), \"\", text)\n",
        "        text = re.sub(r\"\\d+\", \"\", text)\n",
        "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    else:\n",
        "        text = text.strip()\n",
        "    return text\n",
        "\n",
        "# 2. Load and Clean Data\n",
        "print(\"Loading dataset...\")\n",
        "df = pd.read_csv('/content/drive/MyDrive/Nlp/cleaned_dataset (1).csv')  # Update path as needed\n",
        "df = df[['English', 'Sinhala', 'Singlish']].dropna().astype(str)\n",
        "\n",
        "print(\"Cleaning data...\")\n",
        "cleaned = [\n",
        "    [clean_text(row['English']), clean_text(row['Sinhala'], True), clean_text(row['Singlish'])]\n",
        "    for _, row in df.iterrows()\n",
        "]\n",
        "text = np.array(cleaned)\n",
        "\n",
        "# Shuffle and split\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(text)\n",
        "split = int(len(text) * 0.9)\n",
        "train, test = text[:split], text[split:]\n",
        "\n",
        "# 3. Tokenization & Sequence Preparation\n",
        "def create_tokenizer(lines):\n",
        "    tokenizer = Tokenizer(oov_token=\"<UNK>\", filters='')\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        "\n",
        "def max_length(lines):\n",
        "    return max(len(line.split()) for line in lines)\n",
        "\n",
        "eng_tokenizer = create_tokenizer(text[:, 0])\n",
        "sinhala_tokenizer = create_tokenizer(text[:, 1])\n",
        "singlish_tokenizer = create_tokenizer(text[:, 2])\n",
        "\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "sinhala_vocab_size = len(sinhala_tokenizer.word_index) + 1\n",
        "singlish_vocab_size = len(singlish_tokenizer.word_index) + 1\n",
        "\n",
        "eng_max_length = max_length(text[:, 0])\n",
        "sinhala_max_length = max_length(text[:, 1])\n",
        "singlish_max_length = max_length(text[:, 2])\n",
        "\n",
        "def encode_sequences(tokenizer, max_length, lines):\n",
        "    X = tokenizer.texts_to_sequences(lines)\n",
        "    return pad_sequences(X, maxlen=max_length, padding='post')\n",
        "\n",
        "trainX = encode_sequences(eng_tokenizer, eng_max_length, train[:, 0])\n",
        "train_sinY = encode_sequences(sinhala_tokenizer, sinhala_max_length, train[:, 1])\n",
        "train_singY = encode_sequences(singlish_tokenizer, singlish_max_length, train[:, 2])\n",
        "\n",
        "testX = encode_sequences(eng_tokenizer, eng_max_length, test[:, 0])\n",
        "test_sinY = encode_sequences(sinhala_tokenizer, sinhala_max_length, test[:, 1])\n",
        "test_singY = encode_sequences(singlish_tokenizer, singlish_max_length, test[:, 2])\n",
        "\n",
        "\n",
        "def define_multitask_model(src_vocab, sin_vocab, sing_vocab, src_len, sin_len, sing_len, n_units):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(src_len,))\n",
        "    enc_emb = Embedding(src_vocab, n_units)(encoder_inputs)\n",
        "    encoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "    encoder_proj = TimeDistributed(Dense(n_units))(encoder_outputs)\n",
        "\n",
        "    # Sinhala Decoder\n",
        "    sin_repeat = RepeatVector(sin_len)(state_h)\n",
        "    sin_decoder_lstm = LSTM(n_units, return_sequences=True)\n",
        "    sin_decoder_outputs = sin_decoder_lstm(sin_repeat, initial_state=[state_h, state_c])\n",
        "    sin_decoder_proj = TimeDistributed(Dense(n_units))(sin_decoder_outputs)\n",
        "    sin_attention = AdditiveAttention(use_scale=True)([sin_decoder_proj, encoder_proj])  # Fixed here\n",
        "    sin_concat = Concatenate()([sin_decoder_outputs, sin_attention])\n",
        "    sin_out = TimeDistributed(Dense(sin_vocab, activation='softmax'), name=\"sinhala_output\")(sin_concat)\n",
        "\n",
        "    # Singlish Decoder\n",
        "    sing_repeat = RepeatVector(sing_len)(state_h)\n",
        "    sing_decoder_lstm = LSTM(n_units, return_sequences=True)\n",
        "    sing_decoder_outputs = sing_decoder_lstm(sing_repeat, initial_state=[state_h, state_c])\n",
        "    sing_decoder_proj = TimeDistributed(Dense(n_units))(sing_decoder_outputs)\n",
        "    sing_attention = AdditiveAttention(use_scale=True)([sing_decoder_proj, encoder_proj])  # Fixed here\n",
        "    sing_concat = Concatenate()([sing_decoder_outputs, sing_attention])\n",
        "    sing_out = TimeDistributed(Dense(sing_vocab, activation='softmax'), name=\"singlish_output\")(sing_concat)\n",
        "\n",
        "    model = Model(inputs=encoder_inputs, outputs=[sin_out, sing_out])\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss={'sinhala_output': 'sparse_categorical_crossentropy',\n",
        "              'singlish_output': 'sparse_categorical_crossentropy'},\n",
        "        metrics={'sinhala_output': 'accuracy', 'singlish_output': 'accuracy'}\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "print(\"Building model...\")\n",
        "model = define_multitask_model(\n",
        "    eng_vocab_size, sinhala_vocab_size, singlish_vocab_size,\n",
        "    eng_max_length, sinhala_max_length, singlish_max_length, 256\n",
        ")\n",
        "\n",
        "# 5. Training\n",
        "callbacks = [\n",
        "    ModelCheckpoint('multitask_nmt_best.h5', monitor='val_loss', save_best_only=True, verbose=1),\n",
        "    EarlyStopping(monitor='val_loss', patience=4, verbose=1, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.5, verbose=1)\n",
        "]\n",
        "\n",
        "print(\"Training model...\")\n",
        "history = model.fit(\n",
        "    trainX,\n",
        "    {'sinhala_output': train_sinY, 'singlish_output': train_singY},\n",
        "    validation_data=(testX, {'sinhala_output': test_sinY, 'singlish_output': test_singY}),\n",
        "    epochs=30,\n",
        "    batch_size=64,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# 6. Save Model and Tokenizers\n",
        "print(\"Saving model and tokenizers...\")\n",
        "model.save('multitask_nmt_final.h5')\n",
        "dump(eng_tokenizer, open('eng_tokenizer.pkl', 'wb'))\n",
        "dump(sinhala_tokenizer, open('sinhala_tokenizer.pkl', 'wb'))\n",
        "dump(singlish_tokenizer, open('singlish_tokenizer.pkl', 'wb'))\n",
        "\n",
        "# 7. Prediction with Confidence Score\n",
        "def word_for_id(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None\n",
        "\n",
        "def predict_sequence_with_confidence(model, source, target_tokenizer, output_name):\n",
        "    prediction = model.predict(source, verbose=0)[0 if output_name == 'sinhala_output' else 1][0]\n",
        "    integers = [np.argmax(vector) for vector in prediction]\n",
        "    probs = [np.max(vector) for vector in prediction]\n",
        "    words = [word_for_id(i, target_tokenizer) for i in integers if word_for_id(i, target_tokenizer)]\n",
        "    confidence = np.mean(probs)\n",
        "    return ' '.join(words), confidence\n",
        "\n",
        "# Example\n",
        "example_eng = ['how are you']\n",
        "source = encode_sequences(eng_tokenizer, eng_max_length, example_eng)\n",
        "sin_pred, sin_conf = predict_sequence_with_confidence(model, source, sinhala_tokenizer, 'sinhala_output')\n",
        "sing_pred, sing_conf = predict_sequence_with_confidence(model, source, singlish_tokenizer, 'singlish_output')\n",
        "\n",
        "print(\"English:\", example_eng[0])\n",
        "print(\"Sinhala Prediction:\", sin_pred)\n",
        "print(\"Sinhala Confidence:\", sin_conf)\n",
        "print(\"Singlish Prediction:\", sing_pred)\n",
        "print(\"Singlish Confidence:\", sing_conf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm38mIZuwcxE",
        "outputId": "91331fd0-41fa-4c1a-c4a4-379371e4f4df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-14T09:17:20.631210Z",
          "iopub.status.busy": "2025-06-14T09:17:20.630579Z",
          "iopub.status.idle": "2025-06-14T09:17:21.050264Z",
          "shell.execute_reply": "2025-06-14T09:17:21.049441Z",
          "shell.execute_reply.started": "2025-06-14T09:17:20.631187Z"
        },
        "id": "Mn3vT4IcwZRm",
        "outputId": "8499cae6-7513-41f1-cf3f-630e1bca51d3",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English: let s see\n",
            "Sinhala Prediction: අපි බලමු\n",
            "Sinhala Confidence: 0.9850395\n",
            "Singlish Prediction: api balamu\n",
            "Singlish Confidence: 0.9888362\n"
          ]
        }
      ],
      "source": [
        "example_eng = ['let s see']\n",
        "source = encode_sequences(eng_tokenizer, eng_max_length, example_eng)\n",
        "sin_pred, sin_conf = predict_sequence_with_confidence(model, source, sinhala_tokenizer, 'sinhala_output')\n",
        "sing_pred, sing_conf = predict_sequence_with_confidence(model, source, singlish_tokenizer, 'singlish_output')\n",
        "\n",
        "print(\"English:\", example_eng[0])\n",
        "print(\"Sinhala Prediction:\", sin_pred)\n",
        "print(\"Sinhala Confidence:\", sin_conf)\n",
        "print(\"Singlish Prediction:\", sing_pred)\n",
        "print(\"Singlish Confidence:\", sing_conf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-14T09:18:01.793218Z",
          "iopub.status.busy": "2025-06-14T09:18:01.792120Z",
          "iopub.status.idle": "2025-06-14T09:18:02.237833Z",
          "shell.execute_reply": "2025-06-14T09:18:02.237116Z",
          "shell.execute_reply.started": "2025-06-14T09:18:01.793170Z"
        },
        "id": "nBao-rixwZRn",
        "outputId": "d596c2ab-4c83-4e2f-ca91-f3686675f48d",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English: i love you\n",
            "Sinhala Prediction: මම ඔයාට ආදරෙයි\n",
            "Sinhala Confidence: 0.98447174\n",
            "Singlish Prediction: mama oyata adareyi\n",
            "Singlish Confidence: 0.9857285\n"
          ]
        }
      ],
      "source": [
        "example_eng = ['i love you']\n",
        "source = encode_sequences(eng_tokenizer, eng_max_length, example_eng)\n",
        "sin_pred, sin_conf = predict_sequence_with_confidence(model, source, sinhala_tokenizer, 'sinhala_output')\n",
        "sing_pred, sing_conf = predict_sequence_with_confidence(model, source, singlish_tokenizer, 'singlish_output')\n",
        "\n",
        "print(\"English:\", example_eng[0])\n",
        "print(\"Sinhala Prediction:\", sin_pred)\n",
        "print(\"Sinhala Confidence:\", sin_conf)\n",
        "print(\"Singlish Prediction:\", sing_pred)\n",
        "print(\"Singlish Confidence:\", sing_conf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-14T09:21:05.672199Z",
          "iopub.status.busy": "2025-06-14T09:21:05.671584Z",
          "iopub.status.idle": "2025-06-14T09:21:06.072770Z",
          "shell.execute_reply": "2025-06-14T09:21:06.071950Z",
          "shell.execute_reply.started": "2025-06-14T09:21:05.672179Z"
        },
        "id": "jRKvfNMdwZRn",
        "outputId": "f340c0c3-3147-4f6c-b20d-e064ec3e821a",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English: i did bianca a favor and it backfired you didn t\n",
            "Sinhala Prediction: මම බියන්කාට උදව්වක් එය එය එය පිටුපෑවේය පිටුපෑවේය පිටුපෑවේය\n",
            "Sinhala Confidence: 0.9462368\n",
            "Singlish Prediction: mama biyankata udawwak kala eya eya eya\n",
            "Singlish Confidence: 0.9397845\n"
          ]
        }
      ],
      "source": [
        "example_eng = ['i did bianca a favor and it backfired you didn t']\n",
        "source = encode_sequences(eng_tokenizer, eng_max_length, example_eng)\n",
        "sin_pred, sin_conf = predict_sequence_with_confidence(model, source, sinhala_tokenizer, 'sinhala_output')\n",
        "sing_pred, sing_conf = predict_sequence_with_confidence(model, source, singlish_tokenizer, 'singlish_output')\n",
        "\n",
        "print(\"English:\", example_eng[0])\n",
        "print(\"Sinhala Prediction:\", sin_pred)\n",
        "print(\"Sinhala Confidence:\", sin_conf)\n",
        "print(\"Singlish Prediction:\", sing_pred)\n",
        "print(\"Singlish Confidence:\", sing_conf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-14T09:22:41.973684Z",
          "iopub.status.busy": "2025-06-14T09:22:41.973228Z",
          "iopub.status.idle": "2025-06-14T09:22:42.354549Z",
          "shell.execute_reply": "2025-06-14T09:22:42.353803Z",
          "shell.execute_reply.started": "2025-06-14T09:22:41.973656Z"
        },
        "id": "WE9W5rkswZRn",
        "outputId": "ee290cfa-8b87-44c0-bc61-54fc18e8373d",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English: we will work with his people we want peace ask the chief if he understands he understands\n",
            "Sinhala Prediction: අපි ජනතාව සමඟ සමඟ අපට අපට අපට අපට තේරෙනවාදැයි තේරෙනවාදැයි තේරෙනවාදැයි තේරෙනවාදැයි විමසන්න\n",
            "Sinhala Confidence: 0.8533376\n",
            "Singlish Prediction: api ohuge samanga samanga api apata apata apata apata pradhaniyata wimasanna wimasanna wimasanna\n",
            "Singlish Confidence: 0.85045516\n"
          ]
        }
      ],
      "source": [
        "example_eng = ['we will work with his people we want peace ask the chief if he understands he understands']\n",
        "source = encode_sequences(eng_tokenizer, eng_max_length, example_eng)\n",
        "sin_pred, sin_conf = predict_sequence_with_confidence(model, source, sinhala_tokenizer, 'sinhala_output')\n",
        "sing_pred, sing_conf = predict_sequence_with_confidence(model, source, singlish_tokenizer, 'singlish_output')\n",
        "\n",
        "print(\"English:\", example_eng[0])\n",
        "print(\"Sinhala Prediction:\", sin_pred)\n",
        "print(\"Sinhala Confidence:\", sin_conf)\n",
        "print(\"Singlish Prediction:\", sing_pred)\n",
        "print(\"Singlish Confidence:\", sing_conf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ahpqyv8uwZRn",
        "outputId": "3e960c50-919e-499c-e397-d56029ae6e50",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from pickle import load\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('/content/drive/MyDrive/Nlp/multitask_nmt_final.h5')\n",
        "\n",
        "# Load the tokenizers\n",
        "with open('/content/drive/MyDrive/Nlp/eng_tokenizer.pkl', 'rb') as f:\n",
        "    eng_tokenizer = load(f)\n",
        "with open('/content/drive/MyDrive/Nlp/sinhala_tokenizer.pkl', 'rb') as f:\n",
        "    sinhala_tokenizer = load(f)\n",
        "with open('/content/drive/MyDrive/Nlp/singlish_tokenizer.pkl', 'rb') as f:\n",
        "    singlish_tokenizer = load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_GxUEN1PwZRo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def clean_text(text, is_sinhala=False):\n",
        "    if not is_sinhala:\n",
        "        text = text.lower()\n",
        "        text = re.sub(r\"[%s]\" % re.escape(string.punctuation), \"\", text)\n",
        "        text = re.sub(r\"\\d+\", \"\", text)\n",
        "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    else:\n",
        "        text = text.strip()\n",
        "    return text\n",
        "\n",
        "def encode_sequences(tokenizer, max_length, lines):\n",
        "    X = tokenizer.texts_to_sequences(lines)\n",
        "    return pad_sequences(X, maxlen=max_length, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19J5jvtH1m2s",
        "outputId": "9e41fd4e-6445-4f6d-adba-bcaa29a6fbc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English: how are you\n",
            "Sinhala Prediction: ඔයා කොහොමද\n",
            "Sinhala Confidence: 0.9845814\n",
            "Singlish Prediction: oya kohomada\n",
            "Singlish Confidence: 0.98286253\n"
          ]
        }
      ],
      "source": [
        "def word_for_id(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None\n",
        "\n",
        "def predict_sequence_with_confidence(model, source, target_tokenizer, output_name):\n",
        "    prediction = model.predict(source, verbose=0)[0 if output_name == 'sinhala_output' else 1][0]\n",
        "    integers = [np.argmax(vector) for vector in prediction]\n",
        "    probs = [np.max(vector) for vector in prediction]\n",
        "    words = [word_for_id(i, target_tokenizer) for i in integers if word_for_id(i, target_tokenizer)]\n",
        "    confidence = np.mean(probs)\n",
        "    return ' '.join(words), confidence\n",
        "\n",
        "# Example usage:\n",
        "example_eng = ['how are you']\n",
        "example_eng_clean = [clean_text(s) for s in example_eng]\n",
        "eng_max_length = model.input_shape[1]  # Or use the value you used during training\n",
        "source = encode_sequences(eng_tokenizer, eng_max_length, example_eng_clean)\n",
        "\n",
        "sin_pred, sin_conf = predict_sequence_with_confidence(model, source, sinhala_tokenizer, 'sinhala_output')\n",
        "sing_pred, sing_conf = predict_sequence_with_confidence(model, source, singlish_tokenizer, 'singlish_output')\n",
        "\n",
        "print(\"English:\", example_eng[0])\n",
        "print(\"Sinhala Prediction:\", sin_pred)\n",
        "print(\"Sinhala Confidence:\", sin_conf)\n",
        "print(\"Singlish Prediction:\", sing_pred)\n",
        "print(\"Singlish Confidence:\", sing_conf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWxCnuXw3IL_",
        "outputId": "08e2df0d-34b8-4d65-bbbb-0c2eddd1a732"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English: let s see\n",
            "Sinhala Prediction: අපි බලමු\n",
            "Sinhala Confidence: 0.9850395\n",
            "Singlish Prediction: api balamu\n",
            "Singlish Confidence: 0.9888362\n"
          ]
        }
      ],
      "source": [
        "example_eng = ['let s see']\n",
        "source = encode_sequences(eng_tokenizer, eng_max_length, example_eng)\n",
        "sin_pred, sin_conf = predict_sequence_with_confidence(model, source, sinhala_tokenizer, 'sinhala_output')\n",
        "sing_pred, sing_conf = predict_sequence_with_confidence(model, source, singlish_tokenizer, 'singlish_output')\n",
        "\n",
        "print(\"English:\", example_eng[0])\n",
        "print(\"Sinhala Prediction:\", sin_pred)\n",
        "print(\"Sinhala Confidence:\", sin_conf)\n",
        "print(\"Singlish Prediction:\", sing_pred)\n",
        "print(\"Singlish Confidence:\", sing_conf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3ee4TdNNvxs",
        "outputId": "bb3e1dad-469f-4b1b-c4f0-5f4d7ec38b88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English: now this\n",
            "Sinhala Prediction: දැන් මේක\n",
            "Sinhala Confidence: 0.9944297\n",
            "Singlish Prediction: den meka\n",
            "Singlish Confidence: 0.99373496\n"
          ]
        }
      ],
      "source": [
        "example_eng = ['now this']\n",
        "source = encode_sequences(eng_tokenizer, eng_max_length, example_eng)\n",
        "sin_pred, sin_conf = predict_sequence_with_confidence(model, source, sinhala_tokenizer, 'sinhala_output')\n",
        "sing_pred, sing_conf = predict_sequence_with_confidence(model, source, singlish_tokenizer, 'singlish_output')\n",
        "\n",
        "print(\"English:\", example_eng[0])\n",
        "print(\"Sinhala Prediction:\", sin_pred)\n",
        "print(\"Sinhala Confidence:\", sin_conf)\n",
        "print(\"Singlish Prediction:\", sing_pred)\n",
        "print(\"Singlish Confidence:\", sing_conf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aq_P-r9MUXLp",
        "outputId": "dfdb3c15-b96b-4efe-9265-01fcd60ed5cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English: i know\n",
            "Sinhala Prediction: මම දන්නවා\n",
            "Sinhala Confidence: 0.99402505\n",
            "Singlish Prediction: mama dannawa\n",
            "Singlish Confidence: 0.995863\n"
          ]
        }
      ],
      "source": [
        "example_eng = ['i know']\n",
        "source = encode_sequences(eng_tokenizer, eng_max_length, example_eng)\n",
        "sin_pred, sin_conf = predict_sequence_with_confidence(model, source, sinhala_tokenizer, 'sinhala_output')\n",
        "sing_pred, sing_conf = predict_sequence_with_confidence(model, source, singlish_tokenizer, 'singlish_output')\n",
        "\n",
        "print(\"English:\", example_eng[0])\n",
        "print(\"Sinhala Prediction:\", sin_pred)\n",
        "print(\"Sinhala Confidence:\", sin_conf)\n",
        "print(\"Singlish Prediction:\", sing_pred)\n",
        "print(\"Singlish Confidence:\", sing_conf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fk6KdQbDUao-",
        "outputId": "07892c3a-b02d-4362-eb55-0783798e6c3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English: why you crying\n",
            "Sinhala Prediction: ඇයි ඔයා අඬන්නේ අඬන්නේ\n",
            "Sinhala Confidence: 0.9693502\n",
            "Singlish Prediction: eyi oya andanne eyi\n",
            "Singlish Confidence: 0.9692954\n"
          ]
        }
      ],
      "source": [
        "example_eng = ['why you crying']\n",
        "source = encode_sequences(eng_tokenizer, eng_max_length, example_eng)\n",
        "sin_pred, sin_conf = predict_sequence_with_confidence(model, source, sinhala_tokenizer, 'sinhala_output')\n",
        "sing_pred, sing_conf = predict_sequence_with_confidence(model, source, singlish_tokenizer, 'singlish_output')\n",
        "\n",
        "print(\"English:\", example_eng[0])\n",
        "print(\"Sinhala Prediction:\", sin_pred)\n",
        "print(\"Sinhala Confidence:\", sin_conf)\n",
        "print(\"Singlish Prediction:\", sing_pred)\n",
        "print(\"Singlish Confidence:\", sing_conf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IeFVA64Uv_0",
        "outputId": "05f6bb64-7fd0-4a51-bf18-95b9c1053e14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English: let s see \n",
            "Sinhala Prediction: අපි බලමු\n",
            "Sinhala Confidence: 0.9850395\n",
            "Singlish Prediction: api balamu\n",
            "Singlish Confidence: 0.9888362\n"
          ]
        }
      ],
      "source": [
        "example_eng = ['let s see ']\n",
        "source = encode_sequences(eng_tokenizer, eng_max_length, example_eng)\n",
        "sin_pred, sin_conf = predict_sequence_with_confidence(model, source, sinhala_tokenizer, 'sinhala_output')\n",
        "sing_pred, sing_conf = predict_sequence_with_confidence(model, source, singlish_tokenizer, 'singlish_output')\n",
        "\n",
        "print(\"English:\", example_eng[0])\n",
        "print(\"Sinhala Prediction:\", sin_pred)\n",
        "print(\"Sinhala Confidence:\", sin_conf)\n",
        "print(\"Singlish Prediction:\", sing_pred)\n",
        "print(\"Singlish Confidence:\", sing_conf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_88DAIDVGz_",
        "outputId": "ae57cc21-d2a3-40d2-dd5c-c26dfbf5871c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English: i know\n",
            "Sinhala Prediction: මම දන්නවා\n",
            "Sinhala Confidence: 0.99402505\n",
            "Singlish Prediction: mama dannawa\n",
            "Singlish Confidence: 0.995863\n"
          ]
        }
      ],
      "source": [
        "example_eng = ['i know']\n",
        "source = encode_sequences(eng_tokenizer, eng_max_length, example_eng)\n",
        "sin_pred, sin_conf = predict_sequence_with_confidence(model, source, sinhala_tokenizer, 'sinhala_output')\n",
        "sing_pred, sing_conf = predict_sequence_with_confidence(model, source, singlish_tokenizer, 'singlish_output')\n",
        "\n",
        "print(\"English:\", example_eng[0])\n",
        "print(\"Sinhala Prediction:\", sin_pred)\n",
        "print(\"Sinhala Confidence:\", sin_conf)\n",
        "print(\"Singlish Prediction:\", sing_pred)\n",
        "print(\"Singlish Confidence:\", sing_conf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAwe1HZTVXr4",
        "outputId": "2debd40d-30a8-4506-b838-4322a7271119"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English: my dog\n",
            "Sinhala Prediction: මගේ බල්ලා\n",
            "Sinhala Confidence: 0.99678594\n",
            "Singlish Prediction: mage balla\n",
            "Singlish Confidence: 0.9986547\n"
          ]
        }
      ],
      "source": [
        "example_eng = ['my dog']\n",
        "source = encode_sequences(eng_tokenizer, eng_max_length, example_eng)\n",
        "sin_pred, sin_conf = predict_sequence_with_confidence(model, source, sinhala_tokenizer, 'sinhala_output')\n",
        "sing_pred, sing_conf = predict_sequence_with_confidence(model, source, singlish_tokenizer, 'singlish_output')\n",
        "\n",
        "print(\"English:\", example_eng[0])\n",
        "print(\"Sinhala Prediction:\", sin_pred)\n",
        "print(\"Sinhala Confidence:\", sin_conf)\n",
        "print(\"Singlish Prediction:\", sing_pred)\n",
        "print(\"Singlish Confidence:\", sing_conf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKRgTzWqeDoX",
        "outputId": "2319ffe9-9718-4fe8-dd1c-d1a95436a3af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English: i eat banana today\n",
            "Sinhala Prediction: මම අද කනවා\n",
            "Sinhala Confidence: 0.9607722\n",
            "Singlish Prediction: mama ada ada\n",
            "Singlish Confidence: 0.9614807\n"
          ]
        }
      ],
      "source": [
        "example_eng = ['i eat banana today']\n",
        "source = encode_sequences(eng_tokenizer, eng_max_length, example_eng)\n",
        "sin_pred, sin_conf = predict_sequence_with_confidence(model, source, sinhala_tokenizer, 'sinhala_output')\n",
        "sing_pred, sing_conf = predict_sequence_with_confidence(model, source, singlish_tokenizer, 'singlish_output')\n",
        "\n",
        "print(\"English:\", example_eng[0])\n",
        "print(\"Sinhala Prediction:\", sin_pred)\n",
        "print(\"Sinhala Confidence:\", sin_conf)\n",
        "print(\"Singlish Prediction:\", sing_pred)\n",
        "print(\"Singlish Confidence:\", sing_conf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lqnzhtWerUD",
        "outputId": "3ba92dfa-1632-4985-af6c-f85ce60a7f86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English: uh yeah we re old friend you and verona\n",
            "Sinhala Prediction: අහ් ඔව් අපි පරණ යාළුවෙක් ඔබයි\n",
            "Sinhala Confidence: 0.9368271\n",
            "Singlish Prediction: ah ow api perani yaluwek obayi\n",
            "Singlish Confidence: 0.93803924\n"
          ]
        }
      ],
      "source": [
        "example_eng = ['uh yeah we re old friend you and verona']\n",
        "source = encode_sequences(eng_tokenizer, eng_max_length, example_eng)\n",
        "sin_pred, sin_conf = predict_sequence_with_confidence(model, source, sinhala_tokenizer, 'sinhala_output')\n",
        "sing_pred, sing_conf = predict_sequence_with_confidence(model, source, singlish_tokenizer, 'singlish_output')\n",
        "\n",
        "print(\"English:\", example_eng[0])\n",
        "print(\"Sinhala Prediction:\", sin_pred)\n",
        "print(\"Sinhala Confidence:\", sin_conf)\n",
        "print(\"Singlish Prediction:\", sing_pred)\n",
        "print(\"Singlish Confidence:\", sing_conf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQ99anfUfI-W"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 5284582,
          "sourceId": 8789903,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 7659824,
          "sourceId": 12162136,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31041,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
