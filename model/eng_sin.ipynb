{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 8789903,
          "sourceType": "datasetVersion",
          "datasetId": 5284582
        },
        {
          "sourceId": 12162136,
          "sourceType": "datasetVersion",
          "datasetId": 7659824
        }
      ],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, RepeatVector, TimeDistributed, Dense, Concatenate, AdditiveAttention\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from pickle import dump\n",
        "import tensorflow as tf\n",
        "\n",
        "# 1. Data Cleaning\n",
        "def clean_text(text, is_sinhala=False):\n",
        "    if not is_sinhala:\n",
        "        text = text.lower()\n",
        "        text = re.sub(r\"[%s]\" % re.escape(string.punctuation), \"\", text)\n",
        "        text = re.sub(r\"\\d+\", \"\", text)\n",
        "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    else:\n",
        "        text = text.strip()\n",
        "    return text\n",
        "\n",
        "# 2. Load and Clean Data\n",
        "print(\"Loading dataset...\")\n",
        "df = pd.read_csv('/content/drive/MyDrive/Nlp/cleaned_dataset (1).csv')  # Update path as needed\n",
        "df = df[['English', 'Sinhala', 'Singlish']].dropna().astype(str)\n",
        "\n",
        "print(\"Cleaning data...\")\n",
        "cleaned = [\n",
        "    [clean_text(row['English']), clean_text(row['Sinhala'], True), clean_text(row['Singlish'])]\n",
        "    for _, row in df.iterrows()\n",
        "]\n",
        "text = np.array(cleaned)\n",
        "\n",
        "# Shuffle and split\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(text)\n",
        "split = int(len(text) * 0.9)\n",
        "train, test = text[:split], text[split:]\n",
        "\n",
        "# 3. Tokenization & Sequence Preparation\n",
        "def create_tokenizer(lines):\n",
        "    tokenizer = Tokenizer(oov_token=\"<UNK>\", filters='')\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        "\n",
        "def max_length(lines):\n",
        "    return max(len(line.split()) for line in lines)\n",
        "\n",
        "eng_tokenizer = create_tokenizer(text[:, 0])\n",
        "sinhala_tokenizer = create_tokenizer(text[:, 1])\n",
        "singlish_tokenizer = create_tokenizer(text[:, 2])\n",
        "\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "sinhala_vocab_size = len(sinhala_tokenizer.word_index) + 1\n",
        "singlish_vocab_size = len(singlish_tokenizer.word_index) + 1\n",
        "\n",
        "eng_max_length = max_length(text[:, 0])\n",
        "sinhala_max_length = max_length(text[:, 1])\n",
        "singlish_max_length = max_length(text[:, 2])\n",
        "\n",
        "def encode_sequences(tokenizer, max_length, lines):\n",
        "    X = tokenizer.texts_to_sequences(lines)\n",
        "    return pad_sequences(X, maxlen=max_length, padding='post')\n",
        "\n",
        "trainX = encode_sequences(eng_tokenizer, eng_max_length, train[:, 0])\n",
        "train_sinY = encode_sequences(sinhala_tokenizer, sinhala_max_length, train[:, 1])\n",
        "train_singY = encode_sequences(singlish_tokenizer, singlish_max_length, train[:, 2])\n",
        "\n",
        "testX = encode_sequences(eng_tokenizer, eng_max_length, test[:, 0])\n",
        "test_sinY = encode_sequences(sinhala_tokenizer, sinhala_max_length, test[:, 1])\n",
        "test_singY = encode_sequences(singlish_tokenizer, singlish_max_length, test[:, 2])\n",
        "\n",
        "# 4. Model Definition with Additive Attention\n",
        "def define_multitask_model(src_vocab, sin_vocab, sing_vocab, src_len, sin_len, sing_len, n_units):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(src_len,))\n",
        "    enc_emb = Embedding(src_vocab, n_units)(encoder_inputs)\n",
        "    encoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "    encoder_proj = TimeDistributed(Dense(n_units))(encoder_outputs)\n",
        "\n",
        "    # Sinhala Decoder\n",
        "    sin_repeat = RepeatVector(sin_len)(state_h)\n",
        "    sin_decoder_lstm = LSTM(n_units, return_sequences=True)\n",
        "    sin_decoder_outputs = sin_decoder_lstm(sin_repeat, initial_state=[state_h, state_c])\n",
        "    sin_decoder_proj = TimeDistributed(Dense(n_units))(sin_decoder_outputs)\n",
        "    sin_attention = AdditiveAttention(use_scale=True)([sin_decoder_proj, encoder_proj])  # Fixed here\n",
        "    sin_concat = Concatenate()([sin_decoder_outputs, sin_attention])\n",
        "    sin_out = TimeDistributed(Dense(sin_vocab, activation='softmax'), name=\"sinhala_output\")(sin_concat)\n",
        "\n",
        "    # Singlish Decoder\n",
        "    sing_repeat = RepeatVector(sing_len)(state_h)\n",
        "    sing_decoder_lstm = LSTM(n_units, return_sequences=True)\n",
        "    sing_decoder_outputs = sing_decoder_lstm(sing_repeat, initial_state=[state_h, state_c])\n",
        "    sing_decoder_proj = TimeDistributed(Dense(n_units))(sing_decoder_outputs)\n",
        "    sing_attention = AdditiveAttention(use_scale=True)([sing_decoder_proj, encoder_proj])  # Fixed here\n",
        "    sing_concat = Concatenate()([sing_decoder_outputs, sing_attention])\n",
        "    sing_out = TimeDistributed(Dense(sing_vocab, activation='softmax'), name=\"singlish_output\")(sing_concat)\n",
        "\n",
        "    model = Model(inputs=encoder_inputs, outputs=[sin_out, sing_out])\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss={'sinhala_output': 'sparse_categorical_crossentropy',\n",
        "              'singlish_output': 'sparse_categorical_crossentropy'},\n",
        "        metrics={'sinhala_output': 'accuracy', 'singlish_output': 'accuracy'}\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "print(\"Building model...\")\n",
        "model = define_multitask_model(\n",
        "    eng_vocab_size, sinhala_vocab_size, singlish_vocab_size,\n",
        "    eng_max_length, sinhala_max_length, singlish_max_length, 256\n",
        ")\n",
        "\n",
        "# 5. Training\n",
        "callbacks = [\n",
        "    ModelCheckpoint('multitask_nmt_best.h5', monitor='val_loss', save_best_only=True, verbose=1),\n",
        "    EarlyStopping(monitor='val_loss', patience=4, verbose=1, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.5, verbose=1)\n",
        "]\n",
        "\n",
        "print(\"Training model...\")\n",
        "history = model.fit(\n",
        "    trainX,\n",
        "    {'sinhala_output': train_sinY, 'singlish_output': train_singY},\n",
        "    validation_data=(testX, {'sinhala_output': test_sinY, 'singlish_output': test_singY}),\n",
        "    epochs=30,\n",
        "    batch_size=64,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# 6. Save Model and Tokenizers\n",
        "print(\"Saving model and tokenizers...\")\n",
        "model.save('multitask_nmt_final.h5')\n",
        "dump(eng_tokenizer, open('eng_tokenizer.pkl', 'wb'))\n",
        "dump(sinhala_tokenizer, open('sinhala_tokenizer.pkl', 'wb'))\n",
        "dump(singlish_tokenizer, open('singlish_tokenizer.pkl', 'wb'))\n",
        "\n",
        "# 7. Prediction with Confidence Score\n",
        "def word_for_id(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None\n",
        "\n",
        "def predict_sequence_with_confidence(model, source, target_tokenizer, output_name):\n",
        "    prediction = model.predict(source, verbose=0)[0 if output_name == 'sinhala_output' else 1][0]\n",
        "    integers = [np.argmax(vector) for vector in prediction]\n",
        "    probs = [np.max(vector) for vector in prediction]\n",
        "    words = [word_for_id(i, target_tokenizer) for i in integers if word_for_id(i, target_tokenizer)]\n",
        "    confidence = np.mean(probs)\n",
        "    return ' '.join(words), confidence\n",
        "\n",
        "# Example\n",
        "example_eng = ['how are you']\n",
        "source = encode_sequences(eng_tokenizer, eng_max_length, example_eng)\n",
        "sin_pred, sin_conf = predict_sequence_with_confidence(model, source, sinhala_tokenizer, 'sinhala_output')\n",
        "sing_pred, sing_conf = predict_sequence_with_confidence(model, source, singlish_tokenizer, 'singlish_output')\n",
        "\n",
        "print(\"English:\", example_eng[0])\n",
        "print(\"Sinhala Prediction:\", sin_pred)\n",
        "print(\"Sinhala Confidence:\", sin_conf)\n",
        "print(\"Singlish Prediction:\", sing_pred)\n",
        "print(\"Singlish Confidence:\", sing_conf)\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T08:06:48.298757Z",
          "iopub.execute_input": "2025-06-14T08:06:48.299341Z",
          "iopub.status.idle": "2025-06-14T09:10:23.128625Z",
          "shell.execute_reply.started": "2025-06-14T08:06:48.299317Z",
          "shell.execute_reply": "2025-06-14T09:10:23.127804Z"
        },
        "id": "83KwQb-BwZRl",
        "outputId": "63d0d378-f015-4d73-8d7d-b1cd9ad2d0e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Loading dataset...\nCleaning data...\nBuilding model...\nTraining model...\nEpoch 1/30\n\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - loss: 4.6171 - singlish_output_accuracy: 0.7948 - singlish_output_loss: 2.2069 - sinhala_output_accuracy: 0.7879 - sinhala_output_loss: 2.4102\nEpoch 1: val_loss improved from inf to 3.12991, saving model to multitask_nmt_best.h5\n\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 470ms/step - loss: 4.6148 - singlish_output_accuracy: 0.7948 - singlish_output_loss: 2.2058 - sinhala_output_accuracy: 0.7879 - sinhala_output_loss: 2.4090 - val_loss: 3.1299 - val_singlish_output_accuracy: 0.8061 - val_singlish_output_loss: 1.4996 - val_sinhala_output_accuracy: 0.7993 - val_sinhala_output_loss: 1.6294 - learning_rate: 0.0010\nEpoch 2/30\n\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - loss: 3.0730 - singlish_output_accuracy: 0.8071 - singlish_output_loss: 1.4768 - sinhala_output_accuracy: 0.8001 - sinhala_output_loss: 1.5962\nEpoch 2: val_loss improved from 3.12991 to 3.07534, saving model to multitask_nmt_best.h5\n\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 459ms/step - loss: 3.0730 - singlish_output_accuracy: 0.8071 - singlish_output_loss: 1.4768 - sinhala_output_accuracy: 0.8001 - sinhala_output_loss: 1.5962 - val_loss: 3.0753 - val_singlish_output_accuracy: 0.8075 - val_singlish_output_loss: 1.4706 - val_sinhala_output_accuracy: 0.8001 - val_sinhala_output_loss: 1.6038 - learning_rate: 0.0010\nEpoch 3/30\n\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 435ms/step - loss: 2.9979 - singlish_output_accuracy: 0.8070 - singlish_output_loss: 1.4406 - sinhala_output_accuracy: 0.7998 - sinhala_output_loss: 1.5573\nEpoch 3: val_loss improved from 3.07534 to 3.03242, saving model to multitask_nmt_best.h5\n\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 459ms/step - loss: 2.9979 - singlish_output_accuracy: 0.8070 - singlish_output_loss: 1.4406 - sinhala_output_accuracy: 0.7998 - sinhala_output_loss: 1.5573 - val_loss: 3.0324 - val_singlish_output_accuracy: 0.8077 - val_singlish_output_loss: 1.4510 - val_sinhala_output_accuracy: 0.8008 - val_sinhala_output_loss: 1.5805 - learning_rate: 0.0010\nEpoch 4/30\n\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - loss: 2.9053 - singlish_output_accuracy: 0.8091 - singlish_output_loss: 1.3971 - sinhala_output_accuracy: 0.8018 - sinhala_output_loss: 1.5082\nEpoch 4: val_loss improved from 3.03242 to 2.97031, saving model to multitask_nmt_best.h5\n\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 459ms/step - loss: 2.9053 - singlish_output_accuracy: 0.8091 - singlish_output_loss: 1.3971 - sinhala_output_accuracy: 0.8018 - sinhala_output_loss: 1.5082 - val_loss: 2.9703 - val_singlish_output_accuracy: 0.8110 - val_singlish_output_loss: 1.4209 - val_sinhala_output_accuracy: 0.8040 - val_sinhala_output_loss: 1.5485 - learning_rate: 0.0010\nEpoch 5/30\n\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - loss: 2.7818 - singlish_output_accuracy: 0.8139 - singlish_output_loss: 1.3384 - sinhala_output_accuracy: 0.8067 - sinhala_output_loss: 1.4434\nEpoch 5: val_loss improved from 2.97031 to 2.84496, saving model to multitask_nmt_best.h5\n\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 459ms/step - loss: 2.7818 - singlish_output_accuracy: 0.8139 - singlish_output_loss: 1.3384 - sinhala_output_accuracy: 0.8067 - sinhala_output_loss: 1.4434 - val_loss: 2.8450 - val_singlish_output_accuracy: 0.8165 - val_singlish_output_loss: 1.3601 - val_sinhala_output_accuracy: 0.8094 - val_sinhala_output_loss: 1.4840 - learning_rate: 0.0010\nEpoch 6/30\n\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - loss: 2.6059 - singlish_output_accuracy: 0.8195 - singlish_output_loss: 1.2563 - sinhala_output_accuracy: 0.8125 - sinhala_output_loss: 1.3496\nEpoch 6: val_loss improved from 2.84496 to 2.76400, saving model to multitask_nmt_best.h5\n\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 459ms/step - loss: 2.6059 - singlish_output_accuracy: 0.8195 - singlish_output_loss: 1.2563 - sinhala_output_accuracy: 0.8125 - sinhala_output_loss: 1.3496 - val_loss: 2.7640 - val_singlish_output_accuracy: 0.8188 - val_singlish_output_loss: 1.3221 - val_sinhala_output_accuracy: 0.8124 - val_sinhala_output_loss: 1.4409 - learning_rate: 0.0010\nEpoch 7/30\n\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - loss: 2.4290 - singlish_output_accuracy: 0.8239 - singlish_output_loss: 1.1727 - sinhala_output_accuracy: 0.8167 - sinhala_output_loss: 1.2563\nEpoch 7: val_loss improved from 2.76400 to 2.68099, saving model to multitask_nmt_best.h5\n\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 459ms/step - loss: 2.4290 - singlish_output_accuracy: 0.8239 - singlish_output_loss: 1.1727 - sinhala_output_accuracy: 0.8167 - sinhala_output_loss: 1.2563 - val_loss: 2.6810 - val_singlish_output_accuracy: 0.8212 - val_singlish_output_loss: 1.2812 - val_sinhala_output_accuracy: 0.8151 - val_sinhala_output_loss: 1.3987 - learning_rate: 0.0010\nEpoch 8/30\n\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - loss: 2.2258 - singlish_output_accuracy: 0.8286 - singlish_output_loss: 1.0778 - sinhala_output_accuracy: 0.8217 - sinhala_output_loss: 1.1480\nEpoch 8: val_loss improved from 2.68099 to 2.59549, saving model to multitask_nmt_best.h5\n\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 459ms/step - loss: 2.2259 - singlish_output_accuracy: 0.8286 - singlish_output_loss: 1.0778 - sinhala_output_accuracy: 0.8217 - sinhala_output_loss: 1.1480 - val_loss: 2.5955 - val_singlish_output_accuracy: 0.8217 - val_singlish_output_loss: 1.2381 - val_sinhala_output_accuracy: 0.8143 - val_sinhala_output_loss: 1.3563 - learning_rate: 0.0010\nEpoch 9/30\n\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - loss: 2.0360 - singlish_output_accuracy: 0.8317 - singlish_output_loss: 0.9893 - sinhala_output_accuracy: 0.8252 - sinhala_output_loss: 1.0467\nEpoch 9: val_loss improved from 2.59549 to 2.53828, saving model to multitask_nmt_best.h5\n\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 459ms/step - loss: 2.0360 - singlish_output_accuracy: 0.8317 - singlish_output_loss: 0.9893 - sinhala_output_accuracy: 0.8252 - sinhala_output_loss: 1.0467 - val_loss: 2.5383 - val_singlish_output_accuracy: 0.8236 - val_singlish_output_loss: 1.2127 - val_sinhala_output_accuracy: 0.8174 - val_sinhala_output_loss: 1.3245 - learning_rate: 0.0010\nEpoch 10/30\n\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - loss: 1.8540 - singlish_output_accuracy: 0.8362 - singlish_output_loss: 0.9057 - sinhala_output_accuracy: 0.8307 - sinhala_output_loss: 0.9484\nEpoch 10: val_loss improved from 2.53828 to 2.51807, saving model to multitask_nmt_best.h5\n\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 459ms/step - loss: 1.8541 - singlish_output_accuracy: 0.8362 - singlish_output_loss: 0.9057 - sinhala_output_accuracy: 0.8307 - sinhala_output_loss: 0.9484 - val_loss: 2.5181 - val_singlish_output_accuracy: 0.8240 - val_singlish_output_loss: 1.2054 - val_sinhala_output_accuracy: 0.8174 - val_sinhala_output_loss: 1.3114 - learning_rate: 0.0010\nEpoch 11/30\n\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - loss: 1.6744 - singlish_output_accuracy: 0.8427 - singlish_output_loss: 0.8212 - sinhala_output_accuracy: 0.8383 - sinhala_output_loss: 0.8532\nEpoch 11: val_loss improved from 2.51807 to 2.47969, saving model to multitask_nmt_best.h5\n\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 459ms/step - loss: 1.6745 - singlish_output_accuracy: 0.8427 - singlish_output_loss: 0.8213 - sinhala_output_accuracy: 0.8383 - sinhala_output_loss: 0.8532 - val_loss: 2.4797 - val_singlish_output_accuracy: 0.8228 - val_singlish_output_loss: 1.1854 - val_sinhala_output_accuracy: 0.8167 - val_sinhala_output_loss: 1.2930 - learning_rate: 0.0010\nEpoch 12/30\n\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - loss: 1.5534 - singlish_output_accuracy: 0.8470 - singlish_output_loss: 0.7635 - sinhala_output_accuracy: 0.8432 - sinhala_output_loss: 0.7898\nEpoch 12: val_loss improved from 2.47969 to 2.47868, saving model to multitask_nmt_best.h5\n\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 459ms/step - loss: 1.5534 - singlish_output_accuracy: 0.8470 - singlish_output_loss: 0.7636 - sinhala_output_accuracy: 0.8432 - sinhala_output_loss: 0.7898 - val_loss: 2.4787 - val_singlish_output_accuracy: 0.8248 - val_singlish_output_loss: 1.1843 - val_sinhala_output_accuracy: 0.8191 - val_sinhala_output_loss: 1.2932 - learning_rate: 0.0010\nEpoch 13/30\n\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - loss: 1.4553 - singlish_output_accuracy: 0.8510 - singlish_output_loss: 0.7156 - sinhala_output_accuracy: 0.8473 - sinhala_output_loss: 0.7397\nEpoch 13: val_loss improved from 2.47868 to 2.46872, saving model to multitask_nmt_best.h5\n\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 459ms/step - loss: 1.4553 - singlish_output_accuracy: 0.8510 - singlish_output_loss: 0.7156 - sinhala_output_accuracy: 0.8473 - sinhala_output_loss: 0.7397 - val_loss: 2.4687 - val_singlish_output_accuracy: 0.8223 - val_singlish_output_loss: 1.1814 - val_sinhala_output_accuracy: 0.8166 - val_sinhala_output_loss: 1.2861 - learning_rate: 0.0010\nEpoch 14/30\n\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - loss: 1.3632 - singlish_output_accuracy: 0.8552 - singlish_output_loss: 0.6706 - sinhala_output_accuracy: 0.8518 - sinhala_output_loss: 0.6926\nEpoch 14: val_loss did not improve from 2.46872\n\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 457ms/step - loss: 1.3633 - singlish_output_accuracy: 0.8552 - singlish_output_loss: 0.6706 - sinhala_output_accuracy: 0.8518 - sinhala_output_loss: 0.6927 - val_loss: 2.4841 - val_singlish_output_accuracy: 0.8232 - val_singlish_output_loss: 1.1887 - val_sinhala_output_accuracy: 0.8171 - val_sinhala_output_loss: 1.2942 - learning_rate: 0.0010\nEpoch 15/30\n\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - loss: 1.2884 - singlish_output_accuracy: 0.8590 - singlish_output_loss: 0.6338 - sinhala_output_accuracy: 0.8557 - sinhala_output_loss: 0.6546\nEpoch 15: val_loss did not improve from 2.46872\n\nEpoch 15: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 457ms/step - loss: 1.2884 - singlish_output_accuracy: 0.8590 - singlish_output_loss: 0.6338 - sinhala_output_accuracy: 0.8557 - sinhala_output_loss: 0.6546 - val_loss: 2.5046 - val_singlish_output_accuracy: 0.8257 - val_singlish_output_loss: 1.1992 - val_sinhala_output_accuracy: 0.8191 - val_sinhala_output_loss: 1.3040 - learning_rate: 0.0010\nEpoch 16/30\n\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 437ms/step - loss: 1.1752 - singlish_output_accuracy: 0.8661 - singlish_output_loss: 0.5786 - sinhala_output_accuracy: 0.8630 - sinhala_output_loss: 0.5966\nEpoch 16: val_loss did not improve from 2.46872\n\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 457ms/step - loss: 1.1752 - singlish_output_accuracy: 0.8661 - singlish_output_loss: 0.5786 - sinhala_output_accuracy: 0.8630 - sinhala_output_loss: 0.5966 - val_loss: 2.5252 - val_singlish_output_accuracy: 0.8233 - val_singlish_output_loss: 1.2107 - val_sinhala_output_accuracy: 0.8173 - val_sinhala_output_loss: 1.3132 - learning_rate: 5.0000e-04\nEpoch 17/30\n\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 437ms/step - loss: 1.0727 - singlish_output_accuracy: 0.8734 - singlish_output_loss: 0.5308 - sinhala_output_accuracy: 0.8715 - sinhala_output_loss: 0.5418\nEpoch 17: val_loss did not improve from 2.46872\n\nEpoch 17: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 457ms/step - loss: 1.0727 - singlish_output_accuracy: 0.8734 - singlish_output_loss: 0.5309 - sinhala_output_accuracy: 0.8715 - sinhala_output_loss: 0.5418 - val_loss: 2.5803 - val_singlish_output_accuracy: 0.8229 - val_singlish_output_loss: 1.2372 - val_sinhala_output_accuracy: 0.8166 - val_sinhala_output_loss: 1.3417 - learning_rate: 5.0000e-04\nEpoch 17: early stopping\nRestoring model weights from the end of the best epoch: 13.\nSaving model and tokenizers...\nEnglish: how are you\nSinhala Prediction: ඔයා කොහොමද\nSinhala Confidence: 0.9845814\nSinglish Prediction: oya kohomada\nSinglish Confidence: 0.98286253\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm38mIZuwcxE",
        "outputId": "3e189e96-7328-44da-f3b4-8956521acd5c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_eng = ['i love you']\n",
        "source = encode_sequences(eng_tokenizer, eng_max_length, example_eng)\n",
        "sin_pred, sin_conf = predict_sequence_with_confidence(model, source, sinhala_tokenizer, 'sinhala_output')\n",
        "sing_pred, sing_conf = predict_sequence_with_confidence(model, source, singlish_tokenizer, 'singlish_output')\n",
        "\n",
        "print(\"English:\", example_eng[0])\n",
        "print(\"Sinhala Prediction:\", sin_pred)\n",
        "print(\"Sinhala Confidence:\", sin_conf)\n",
        "print(\"Singlish Prediction:\", sing_pred)\n",
        "print(\"Singlish Confidence:\", sing_conf)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T09:18:01.792120Z",
          "iopub.execute_input": "2025-06-14T09:18:01.793218Z",
          "iopub.status.idle": "2025-06-14T09:18:02.237833Z",
          "shell.execute_reply.started": "2025-06-14T09:18:01.793170Z",
          "shell.execute_reply": "2025-06-14T09:18:02.237116Z"
        },
        "id": "nBao-rixwZRn",
        "outputId": "d596c2ab-4c83-4e2f-ca91-f3686675f48d"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "English: i love you\nSinhala Prediction: මම ඔයාට ආදරෙයි\nSinhala Confidence: 0.98447174\nSinglish Prediction: mama oyata adareyi\nSinglish Confidence: 0.9857285\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from pickle import load\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('/content/drive/MyDrive/Nlp/multitask_nmt_final.h5')\n",
        "\n",
        "# Load the tokenizers\n",
        "with open('/content/drive/MyDrive/Nlp/eng_tokenizer.pkl', 'rb') as f:\n",
        "    eng_tokenizer = load(f)\n",
        "with open('/content/drive/MyDrive/Nlp/sinhala_tokenizer.pkl', 'rb') as f:\n",
        "    sinhala_tokenizer = load(f)\n",
        "with open('/content/drive/MyDrive/Nlp/singlish_tokenizer.pkl', 'rb') as f:\n",
        "    singlish_tokenizer = load(f)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ahpqyv8uwZRn",
        "outputId": "02c2eea2-81b6-4671-de00-fd2cf534a980"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def clean_text(text, is_sinhala=False):\n",
        "    if not is_sinhala:\n",
        "        text = text.lower()\n",
        "        text = re.sub(r\"[%s]\" % re.escape(string.punctuation), \"\", text)\n",
        "        text = re.sub(r\"\\d+\", \"\", text)\n",
        "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    else:\n",
        "        text = text.strip()\n",
        "    return text\n",
        "\n",
        "def encode_sequences(tokenizer, max_length, lines):\n",
        "    X = tokenizer.texts_to_sequences(lines)\n",
        "    return pad_sequences(X, maxlen=max_length, padding='post')"
      ],
      "metadata": {
        "trusted": true,
        "id": "_GxUEN1PwZRo"
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "def word_for_id(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None\n",
        "\n",
        "def predict_sequence_with_confidence(model, source, target_tokenizer, output_name):\n",
        "    prediction = model.predict(source, verbose=0)[0 if output_name == 'sinhala_output' else 1][0]\n",
        "    integers = [np.argmax(vector) for vector in prediction]\n",
        "    probs = [np.max(vector) for vector in prediction]\n",
        "    words = [word_for_id(i, target_tokenizer) for i in integers if word_for_id(i, target_tokenizer)]\n",
        "    confidence = np.mean(probs)\n",
        "    return ' '.join(words), confidence\n",
        "\n",
        "# Example usage:\n",
        "example_eng = ['let s see']\n",
        "example_eng_clean = [clean_text(s) for s in example_eng]\n",
        "eng_max_length = model.input_shape[1]  # Or use the value you used during training\n",
        "source = encode_sequences(eng_tokenizer, eng_max_length, example_eng_clean)\n",
        "\n",
        "sin_pred, sin_conf = predict_sequence_with_confidence(model, source, sinhala_tokenizer, 'sinhala_output')\n",
        "sing_pred, sing_conf = predict_sequence_with_confidence(model, source, singlish_tokenizer, 'singlish_output')\n",
        "\n",
        "print(\"English:\", example_eng[0])\n",
        "print(\"Sinhala Prediction:\", sin_pred)\n",
        "print(\"Sinhala Confidence:\", sin_conf)\n",
        "print(\"Singlish Prediction:\", sing_pred)\n",
        "print(\"Singlish Confidence:\", sing_conf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19J5jvtH1m2s",
        "outputId": "94ce5a4d-f593-4d70-b570-ece1f326d9ae"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: let s see\n",
            "Sinhala Prediction: අපි බලමු\n",
            "Sinhala Confidence: 0.9850395\n",
            "Singlish Prediction: api balamu\n",
            "Singlish Confidence: 0.9888362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_eng = ['she already know']\n",
        "source = encode_sequences(eng_tokenizer, eng_max_length, example_eng)\n",
        "sin_pred, sin_conf = predict_sequence_with_confidence(model, source, sinhala_tokenizer, 'sinhala_output')\n",
        "sing_pred, sing_conf = predict_sequence_with_confidence(model, source, singlish_tokenizer, 'singlish_output')\n",
        "\n",
        "print(\"English:\", example_eng[0])\n",
        "print(\"Sinhala Prediction:\", sin_pred)\n",
        "print(\"Sinhala Confidence:\", sin_conf)\n",
        "print(\"Singlish Prediction:\", sing_pred)\n",
        "print(\"Singlish Confidence:\", sing_conf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWxCnuXw3IL_",
        "outputId": "60140f0b-381d-42ad-af0c-309e0756decb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: she already know\n",
            "Sinhala Prediction: ඇය දැනටමත් දන්නවා\n",
            "Sinhala Confidence: 0.9875726\n",
            "Singlish Prediction: eya denatamath dannawa\n",
            "Singlish Confidence: 0.9853925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_eng = ['now this']\n",
        "source = encode_sequences(eng_tokenizer, eng_max_length, example_eng)\n",
        "sin_pred, sin_conf = predict_sequence_with_confidence(model, source, sinhala_tokenizer, 'sinhala_output')\n",
        "sing_pred, sing_conf = predict_sequence_with_confidence(model, source, singlish_tokenizer, 'singlish_output')\n",
        "\n",
        "print(\"English:\", example_eng[0])\n",
        "print(\"Sinhala Prediction:\", sin_pred)\n",
        "print(\"Sinhala Confidence:\", sin_conf)\n",
        "print(\"Singlish Prediction:\", sing_pred)\n",
        "print(\"Singlish Confidence:\", sing_conf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3ee4TdNNvxs",
        "outputId": "bb3e1dad-469f-4b1b-c4f0-5f4d7ec38b88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: now this\n",
            "Sinhala Prediction: දැන් මේක\n",
            "Sinhala Confidence: 0.9944297\n",
            "Singlish Prediction: den meka\n",
            "Singlish Confidence: 0.99373496\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_eng = ['i know']\n",
        "source = encode_sequences(eng_tokenizer, eng_max_length, example_eng)\n",
        "sin_pred, sin_conf = predict_sequence_with_confidence(model, source, sinhala_tokenizer, 'sinhala_output')\n",
        "sing_pred, sing_conf = predict_sequence_with_confidence(model, source, singlish_tokenizer, 'singlish_output')\n",
        "\n",
        "print(\"English:\", example_eng[0])\n",
        "print(\"Sinhala Prediction:\", sin_pred)\n",
        "print(\"Sinhala Confidence:\", sin_conf)\n",
        "print(\"Singlish Prediction:\", sing_pred)\n",
        "print(\"Singlish Confidence:\", sing_conf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aq_P-r9MUXLp",
        "outputId": "dfdb3c15-b96b-4efe-9265-01fcd60ed5cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: i know\n",
            "Sinhala Prediction: මම දන්නවා\n",
            "Sinhala Confidence: 0.99402505\n",
            "Singlish Prediction: mama dannawa\n",
            "Singlish Confidence: 0.995863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_eng = ['my home']\n",
        "source = encode_sequences(eng_tokenizer, eng_max_length, example_eng)\n",
        "sin_pred, sin_conf = predict_sequence_with_confidence(model, source, sinhala_tokenizer, 'sinhala_output')\n",
        "sing_pred, sing_conf = predict_sequence_with_confidence(model, source, singlish_tokenizer, 'singlish_output')\n",
        "\n",
        "print(\"English:\", example_eng[0])\n",
        "print(\"Sinhala Prediction:\", sin_pred)\n",
        "print(\"Sinhala Confidence:\", sin_conf)\n",
        "print(\"Singlish Prediction:\", sing_pred)\n",
        "print(\"Singlish Confidence:\", sing_conf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fk6KdQbDUao-",
        "outputId": "d1048f15-b7cf-41b9-d489-c68a000913e5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: my home\n",
            "Sinhala Prediction: මගේ ගෙදර\n",
            "Sinhala Confidence: 0.9987499\n",
            "Singlish Prediction: mage gedara\n",
            "Singlish Confidence: 0.9980909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_eng = ['let s see ']\n",
        "source = encode_sequences(eng_tokenizer, eng_max_length, example_eng)\n",
        "sin_pred, sin_conf = predict_sequence_with_confidence(model, source, sinhala_tokenizer, 'sinhala_output')\n",
        "sing_pred, sing_conf = predict_sequence_with_confidence(model, source, singlish_tokenizer, 'singlish_output')\n",
        "\n",
        "print(\"English:\", example_eng[0])\n",
        "print(\"Sinhala Prediction:\", sin_pred)\n",
        "print(\"Sinhala Confidence:\", sin_conf)\n",
        "print(\"Singlish Prediction:\", sing_pred)\n",
        "print(\"Singlish Confidence:\", sing_conf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IeFVA64Uv_0",
        "outputId": "112e2d4e-bf4b-4a3c-d693-5c94c6be3317"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: let s see \n",
            "Sinhala Prediction: අපි බලමු\n",
            "Sinhala Confidence: 0.9850395\n",
            "Singlish Prediction: api balamu\n",
            "Singlish Confidence: 0.9888362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_eng = ['i know']\n",
        "source = encode_sequences(eng_tokenizer, eng_max_length, example_eng)\n",
        "sin_pred, sin_conf = predict_sequence_with_confidence(model, source, sinhala_tokenizer, 'sinhala_output')\n",
        "sing_pred, sing_conf = predict_sequence_with_confidence(model, source, singlish_tokenizer, 'singlish_output')\n",
        "\n",
        "print(\"English:\", example_eng[0])\n",
        "print(\"Sinhala Prediction:\", sin_pred)\n",
        "print(\"Sinhala Confidence:\", sin_conf)\n",
        "print(\"Singlish Prediction:\", sing_pred)\n",
        "print(\"Singlish Confidence:\", sing_conf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_88DAIDVGz_",
        "outputId": "ae57cc21-d2a3-40d2-dd5c-c26dfbf5871c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: i know\n",
            "Sinhala Prediction: මම දන්නවා\n",
            "Sinhala Confidence: 0.99402505\n",
            "Singlish Prediction: mama dannawa\n",
            "Singlish Confidence: 0.995863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_eng = ['her dog']\n",
        "source = encode_sequences(eng_tokenizer, eng_max_length, example_eng)\n",
        "sin_pred, sin_conf = predict_sequence_with_confidence(model, source, sinhala_tokenizer, 'sinhala_output')\n",
        "sing_pred, sing_conf = predict_sequence_with_confidence(model, source, singlish_tokenizer, 'singlish_output')\n",
        "\n",
        "print(\"English:\", example_eng[0])\n",
        "print(\"Sinhala Prediction:\", sin_pred)\n",
        "print(\"Sinhala Confidence:\", sin_conf)\n",
        "print(\"Singlish Prediction:\", sing_pred)\n",
        "print(\"Singlish Confidence:\", sing_conf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAwe1HZTVXr4",
        "outputId": "8937e849-d5b4-41b2-f2ce-d660825c4eaf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: her dog\n",
            "Sinhala Prediction: ඇගේ බල්ලා\n",
            "Sinhala Confidence: 0.99123794\n",
            "Singlish Prediction: ege balla\n",
            "Singlish Confidence: 0.99605316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jQ99anfUfI-W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}